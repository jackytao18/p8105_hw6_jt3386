---
title: "Solutions for Homework 6"
author: "Jiajun Tao"
date: "2022-11-29"
output: github_document
---

```{r, include = FALSE,message=FALSE,warning=FALSE}
library(tidyverse)
library(modelr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	message = FALSE,
  fig.width = 8,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


### Problem 1

First, we download the data.

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```


### Problem 2

First, we import the data.

```{r}
homicides_df = read_csv("data/homicide-data.csv") 

homicides_df
```

Then we created a `city_state` variable, and a binary variable indicating whether the homicide is solved. Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO – these don’t report victim race. Also omit Tulsa, AL – this is a data entry mistake. Limit the analysis those for whom `victim_race` is white or black. Be sure that `victim_age` is numeric.

```{r}
homicides_df=
  homicides_df %>% 
  mutate(
    city_state = str_c(city, ", ", state),
    whether_solved = ifelse(disposition %in% c("Closed without arrest", "Open/No arrest"), 0, 1)
  ) %>% 
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")) %>% 
  filter(victim_race %in% c("Black", "White")) %>% 
  filter(victim_age != "Unknown") %>% 
  mutate(victim_age = as.numeric(victim_age),
         victim_sex = as.factor(victim_sex),
         victim_race = fct_relevel(victim_race,"White"))

homicides_df
```

For the city of Baltimore, MD, use the `glm` function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors. Save the output of `glm` as an R object; apply the `broom::tidy` to this object; and obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.

```{r}
fit_logistic = 
  homicides_df %>% 
  filter(city == "Baltimore") %>% 
  glm(whether_solved ~ victim_age + victim_sex + victim_race, data = ., family = binomial())

fit_logistic %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate),
         lower = exp(confint(fit_logistic)[,1]),
         upper = exp(confint(fit_logistic)[,2])) %>% 
  filter(term == "victim_sexMale") %>% 
  select(OR, lower, upper) %>% 
  knitr::kable(digits = 3)
```

We can see that homicides in which the victim is male are significantly less like to be resolved than those in which the victim is female in Baltimore.

Now run `glm` for each of the cities in the dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. 

```{r}
logistic_function = function(demographics){
  
  logistic_fit = glm(whether_solved ~ victim_age + victim_sex + victim_race, data = demographics, family = binomial())
  
  logistic_fit %>% 
    broom::tidy() %>% 
    mutate(OR = exp(estimate),
           lower = exp(confint(logistic_fit)[,1]),
           upper = exp(confint(logistic_fit)[,2])) %>% 
    filter(term == "victim_sexMale") %>% 
    select(OR, lower, upper) 
}

logistic_df = 
  homicides_df %>% 
  select(city_state,victim_age, victim_race, victim_sex, whether_solved) %>% 
  nest(demographics = victim_age:whether_solved) %>% 
  mutate(
    logistic_fit = purrr::map(demographics, logistic_function)
  ) %>% 
  select(-demographics) %>% 
  unnest(logistic_fit)

logistic_df
```

Finally we create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR.

```{r}
logistic_df %>% 
  mutate(
    city_state = fct_reorder(city_state, OR)
  ) %>% 
  ggplot(aes(x = city_state, y = OR)) +
  geom_errorbar(aes(ymin = lower, ymax = upper)) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  labs(
    title = "Odds ratio for solving homicides comparing male victims to female victims",
    x = "City",
    y = "Estimated OR"
  )
```

The estimated OR varies among the cities,the lowest OR is in New York, which indicates that in New York, homicides in which the victim is male are significantly less like to be resolved than those in which the victim is female. However, the highest OR is in Albuquerque, which indicates that homicides in which the victim is male are more likely to be resolved than those in which the victim is female. As we can see some intervals are really wide because the sample size is quite small. So we should investigate more before jumping to conclusion.

### Problem 3

First, we load the data.

```{r}
bw_df = 
  read_csv("data/birthweight.csv") %>% 
  mutate(
     babysex = factor(babysex,levels = c(1, 2),labels = c("Male", "Female")),
         frace = factor(frace, levels = c(1, 2, 3, 4, 8, 9),labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown")), 
         malform = factor(malform,levels = c(0, 1),labels = c("Absent", "Present")),
         mrace = factor(mrace,levels = c(1, 2, 3, 4, 8),labels = c("White", "Black", "Asian", "Puerto Rican", "Other"))
  ) %>% 
  na.omit() 

bw_df
```

After skimming the data, I propose that `babysex`, `bhead`, `blength`, `gaweeks`, `malform`, `smoken`, `wtgain` would contribute most to the `bwt`. I think a baby's characteristics would affect his weight, so I put in all variables about babies. Then I think gestational age, mother smoking, and mother's gained weight are also important. Finally, I think malformation could influence a baby's weight. So it is my first model.

```{r}
fit_model = lm(bwt ~ babysex + bhead + blength + gaweeks + malform + smoken + wtgain, data = bw_df)

summary(fit_model)
```

We can see that `malform` variable is not that statistically significant, so we would like to remove it.

```{r}
fit_model = lm(bwt ~ babysex + bhead + blength + gaweeks + smoken + wtgain, data = bw_df)

summary(fit_model)
```

Then it looks great. We plot residuals against fitted values.

```{r}
bw_df %>% 
  add_residuals(fit_model) %>% 
  add_predictions(fit_model) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point() +
  labs(
    x = "Fitted Values",
    y = "Residuals"
  )
```

The residuals are around 0, and the fitted values are around 3000. There are several extreme points, but that does not matter.

Then we compare the models using cross validation.

```{r}
cv_df = 
  crossv_mc(bw_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

cv_df = 
  cv_df %>% 
  mutate(
    model_1  = map(.x = train, ~lm(bwt ~ blength + gaweeks, data = bw_df)),
    model_2  = map(.x = train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + blength*babysex + bhead*babysex + bhead*blength*babysex, data = bw_df)),
    my_model  = map(.x = train, ~lm(bwt ~ babysex + bhead + blength + gaweeks + smoken + wtgain, data = bw_df))) %>% 
  mutate(
    rmse_model_1 = map2_dbl(model_1, test, ~rmse(model = .x, data = .y)),
    rmse_model_2 = map2_dbl(model_2, test, ~rmse(model = .x, data = .y)),
    rmse_my_model = map2_dbl(my_model, test, ~rmse(model = .x, data = .y)))
```

Finally, we make a plot to show the distribution of RMSE

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_violin()
```

As we can see, my model is slightly better than model 2, and might be the best among the three models.