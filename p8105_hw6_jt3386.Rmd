---
title: "Solutions for Homework 6"
author: "Jiajun Tao"
date: "2022-11-29"
output: github_document
---

```{r, include = FALSE,message=FALSE,warning=FALSE}
library(tidyverse)
library(modelr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	message = FALSE,
  fig.width = 8,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


### Problem 1

First, we download the data.

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

Then we use bootstrap to draw the samples, and use `bloom::glance` or `bloom::tidy` to extract the value.

```{r}
result_df =
  weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    model = map(strap, ~lm(tmax ~ tmin, data = .x)),
    r_results = map(model, broom::glance),
    beta_results = map(model, broom::tidy)
  ) 
```

We make a plot to show the distribution of r_squared.

```{r}
result_df %>% 
  unnest(r_results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

As we can see in the plot, it's approximately normal. The mean for $\hat{r}^2$ is about 0.91.

Then we want to take the 2.5% and 97.5% quantiles of estimates to construct a 95% confidence interval.

```{r}
result_df %>% 
  unnest(r_results) %>%
  select(r.squared) %>% 
  summarise(
    lower = quantile(r.squared, .025),
    upper = quantile(r.squared, .975)
  ) %>% 
  knitr::kable(caption = "95% CI for R-squared", digits = 3)
```

We make a plot to show the distribution for $\log(\beta_0 * \beta1)$.

```{r}
beta_df =
  result_df %>% 
  unnest(beta_results) %>% 
  select(.id, term, estimate) %>% 
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) %>% 
  rename(beta_0 = `(Intercept)`, beta_1 = tmin) %>% 
  mutate(log_b0b1 = log(beta_0 * beta_1))

ggplot(beta_df, aes(x = log_b0b1)) + geom_density()
```

Also it's approximately normal. The mean for $\hat{r}^2$ is about 2.00.

And we take the 2.5% and 97.5% quantiles of estimates to construct a 95% confidence interval.

```{r}
beta_df %>% 
  summarise(
    lower = quantile(log_b0b1, .025),
    upper = quantile(log_b0b1, .975)
  ) %>% 
  knitr::kable(caption = "95% CI for log_b0b1", digits = 3)
```

### Problem 2

First, we import the data.

```{r}
homicides_df = read_csv("data/homicide-data.csv") 

homicides_df
```

Then we created a `city_state` variable, and a binary variable indicating whether the homicide is solved. Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO – these don’t report victim race. Also omit Tulsa, AL – this is a data entry mistake. Limit the analysis those for whom `victim_race` is white or black. Be sure that `victim_age` is numeric.

```{r}
homicides_df=
  homicides_df %>% 
  mutate(
    city_state = str_c(city, ", ", state),
    whether_solved = ifelse(disposition %in% c("Closed without arrest", "Open/No arrest"), 0, 1)
  ) %>% 
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")) %>% 
  filter(victim_race %in% c("Black", "White")) %>% 
  filter(victim_age != "Unknown") %>% 
  mutate(victim_age = as.numeric(victim_age),
         victim_sex = as.factor(victim_sex),
         victim_race = fct_relevel(victim_race,"White"))

homicides_df
```

For the city of Baltimore, MD, use the `glm` function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors. Save the output of `glm` as an R object; apply the `broom::tidy` to this object; and obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.

```{r}
fit_logistic = 
  homicides_df %>% 
  filter(city == "Baltimore") %>% 
  glm(whether_solved ~ victim_age + victim_sex + victim_race, data = ., family = binomial())

fit_logistic %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate),
         lower = exp(confint(fit_logistic)[,1]),
         upper = exp(confint(fit_logistic)[,2])) %>% 
  filter(term == "victim_sexMale") %>% 
  select(OR, lower, upper) %>% 
  knitr::kable(digits = 3)
```

We can see that homicides in which the victim is male are significantly less like to be resolved than those in which the victim is female in Baltimore.

Now run `glm` for each of the cities in the dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. 

```{r}
logistic_function = function(demographics){
  
  logistic_fit = glm(whether_solved ~ victim_age + victim_sex + victim_race, data = demographics, family = binomial())
  
  logistic_fit %>% 
    broom::tidy() %>% 
    mutate(OR = exp(estimate),
           lower = exp(confint(logistic_fit)[,1]),
           upper = exp(confint(logistic_fit)[,2])) %>% 
    filter(term == "victim_sexMale") %>% 
    select(OR, lower, upper) 
}

logistic_df = 
  homicides_df %>% 
  select(city_state,victim_age, victim_race, victim_sex, whether_solved) %>% 
  nest(demographics = victim_age:whether_solved) %>% 
  mutate(
    logistic_fit = purrr::map(demographics, logistic_function)
  ) %>% 
  select(-demographics) %>% 
  unnest(logistic_fit)

logistic_df
```

Finally we create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR.

```{r}
logistic_df %>% 
  mutate(
    city_state = fct_reorder(city_state, OR)
  ) %>% 
  ggplot(aes(x = city_state, y = OR)) +
  geom_errorbar(aes(ymin = lower, ymax = upper)) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  labs(
    title = "Odds ratio for solving homicides comparing male victims to female victims",
    x = "City",
    y = "Estimated OR"
  )
```

The estimated OR varies among the cities. The lowest OR is in New York, and the upper limit doesn't exceed 1, which indicates that in New York, homicides in which the victim is male are significantly less like to be resolved than those in which the victim is female. However, the highest OR is in Albuquerque, which indicates that homicides in which the victim is male are more likely to be resolved than those in which the victim is female. As we can see some confidence intervals are really wide because the sample size is quite small. There may be some relationships between sex and disposition, but we should investigate more before jumping to conclusion.

### Problem 3

First, we load the data.

```{r}
bw_df = 
  read_csv("data/birthweight.csv") %>% 
  mutate(
     babysex = factor(babysex,levels = c(1, 2),labels = c("Male", "Female")),
         frace = factor(frace, levels = c(1, 2, 3, 4, 8, 9),labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown")), 
         malform = factor(malform,levels = c(0, 1),labels = c("Absent", "Present")),
         mrace = factor(mrace,levels = c(1, 2, 3, 4, 8),labels = c("White", "Black", "Asian", "Puerto Rican", "Other"))
  ) %>% 
  na.omit() 

bw_df
```

My strategy is to fit the full model first, and then use backward method to select the variables.

```{r}
full_model = lm(bwt ~ ., data = bw_df)
final_model = step(full_model, direction = "backward")
final_model$call
```


```{r}
bw_df %>% 
  add_residuals(final_model) %>% 
  add_predictions(final_model) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point() +
  labs(
    x = "Fitted Values",
    y = "Residuals"
  )
```

The residuals are around 0, and the fitted values are around 3000. There are several extreme points, but that does not matter.

Then we compare the models using cross validation.

```{r}
cv_df = 
  crossv_mc(bw_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

cv_df = 
  cv_df %>% 
  mutate(
    model_1  = map(.x = train, ~lm(bwt ~ blength + gaweeks, data = bw_df)),
    model_2  = map(.x = train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + blength*babysex + bhead*babysex + bhead*blength*babysex, data = bw_df)),
    my_model  = map(.x = train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = bw_df))) %>% 
  mutate(
    rmse_model_1 = map2_dbl(model_1, test, ~rmse(model = .x, data = .y)),
    rmse_model_2 = map2_dbl(model_2, test, ~rmse(model = .x, data = .y)),
    rmse_my_model = map2_dbl(my_model, test, ~rmse(model = .x, data = .y)))
```

Finally, we make a plot to show the distribution of RMSE

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_violin()
```

As we can see, my model's RMSE is the smallest, it's better than model 2, and might be the best among the three models.